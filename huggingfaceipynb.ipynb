{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXy2gKISGeJ9yJPuoouwji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilpate3345/deep-learning/blob/main/huggingfaceipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDTqdKQtInwK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment analysis\n",
        "from transformers import pipeline\n",
        "classifier=pipeline(\"sentiment-analysis\")\n",
        "result=classifier(\"i was so not happy with last mission impossible movie\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0vXaPp7_I6Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task=\"sentiment-analysis\")(\"i was confused with the barbie movie\")"
      ],
      "metadata": {
        "id": "qVqky6S3NlVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task=\"sentiment-analysis\")\n",
        "result = classifier(\n",
        "    \"Everyday lots of LLM papers are published about LLMs evaluation. \"\n",
        "    \"Lots of them look very promising. \"\n",
        "    \"I am not sure if we can actually evaluate LLMs.\"\n",
        ")\n",
        "\n",
        "# Print result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "mL3wJxrWOK_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "result = classifier(\n",
        "    \"Everyday lots of LLM papers are published about LLMs evaluation. \"\n",
        "    \"Lots of them look very promising. \"\n",
        "    \"I am not sure if we can actually evaluate LLMs.\"\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "fMFeSo3MPQnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#batch sentiment snalysis\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lgy5CHfuQiIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=pipeline(task=\"sentiment-analysis\")\n",
        "task_list=[\"i really like autoencoder,best models for anomaly detection\",\\\n",
        "           \"i am not sure if we can actually evaluate llm\",\\\n",
        "           \"passiveAgressive is the name of a linear Regression model that so many people donot know.\",\\\n",
        "           \"i hate long meetings.\"]\n",
        "classifier(task_list)"
      ],
      "metadata": {
        "id": "1XJZ3jauQnYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=pipeline(task=\"sentiment-analysis\",model=\"samlowe/roberta-base-go_emotions\")\n",
        "task_list=[\"i really like autoencoder,best models for anomaly detection\",\\\n",
        "           \"i am not sure if we can actually evaluate llm\",\\\n",
        "           \"passiveAgressive is the name of a linear Regression model that so many people donot know.\",\\\n",
        "           \"i hate long meetings.\"]\n",
        "classifier(task_list)"
      ],
      "metadata": {
        "id": "V7pCOXsjRWek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrVs6jBAR9kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Generation"
      ],
      "metadata": {
        "id": "Twr5Ccu1SClR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load text generation pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "# Generate text\n",
        "generated_text = text_generator(\n",
        "    \"Today is a rainy day in London\",\n",
        "    max_length=50,          # controls output length\n",
        "    num_return_sequences=2, # plural\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Print first generated text\n",
        "print(\"Generated text:\\n\", generated_text[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "8QWhzdUZSGJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmOG6_DGTH-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Answering"
      ],
      "metadata": {
        "id": "xkfex1kbTR8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "qa_model=pipeline(\"question-answering\")\n",
        "question=\"what is my job?\"\n",
        "context=\"i am developing AI model with python\"\n",
        "qa_model(question=question,context=context)\n"
      ],
      "metadata": {
        "id": "oFklrBNzTWgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qm6EG5AUUiEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "4bofsz2oUj6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# --- First model ---\n",
        "model_name1 = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "myTokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "myModel1 = AutoModelForSequenceClassification.from_pretrained(model_name1)\n",
        "\n",
        "# Create pipeline\n",
        "classifier1 = pipeline(\"sentiment-analysis\", model=myModel1, tokenizer=myTokenizer1)\n",
        "\n",
        "# Test it\n",
        "res1 = classifier1(\"I was so not happy with the Barbie movie.\")\n",
        "print(\"Result from DistilBERT model:\", res1)\n",
        "\n",
        "\n",
        "# --- Second model ---\n",
        "model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "myTokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "myModel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
        "\n",
        "# Create pipeline\n",
        "classifier2 = pipeline(\"sentiment-analysis\", model=myModel2, tokenizer=myTokenizer2)\n",
        "\n",
        "# Test it\n",
        "res2 = classifier2(\"I was so not happy with the Spiderman movie.\")\n",
        "print(\"Result from Multilingual BERT model:\", res2)\n"
      ],
      "metadata": {
        "id": "O0hJdSw9Uh_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Your input text\n",
        "text = \"I love the Barbie movie!\"\n",
        "\n",
        "#  Convert text → tokens (IDs)\n",
        "encoded = tokenizer.encode(text)\n",
        "print(\"Encoded (token IDs):\", encoded)\n",
        "\n",
        "#  Convert token IDs → readable tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens → back to text\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(\"Decoded text:\", decoded)\n"
      ],
      "metadata": {
        "id": "8ajOMy8PXq2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import library\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Step 2: Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Step 3: Give input text\n",
        "text = \"I love watching Barbie and Spiderman movies!\"\n",
        "\n",
        "# Step 4: Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PqXYDX12X-BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ],
      "metadata": {
        "id": "GOvSSE8lYKDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode token IDs back to readable text\n",
        "decoded_text = tokenizer.decode(token_ids)  # Make sure variable name matches\n",
        "print(\"Decoded text:\", decoded_text)"
      ],
      "metadata": {
        "id": "nzDQcaM_YUNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tunings"
      ],
      "metadata": {
        "id": "KO1vz6kEZvw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "BPx_QP26ZzRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load\n",
        "from datasets import load_dataset\n",
        "dataset=load_dataset('imdb')"
      ],
      "metadata": {
        "id": "nwHjkWc3Z42p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "sguwJTrra1zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "T0ay2dZabB-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing task"
      ],
      "metadata": {
        "id": "MgbL6xTXbD4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "#tokenize the datasets\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['text'],padding=\"max_length\",truncation=True)\n",
        "tokenized_datasets=dataset.map(tokenize_function,batched=True)"
      ],
      "metadata": {
        "id": "YI8wBxXhbI21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "m5JHTaGlcu33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 4:set up the training Arguments"
      ],
      "metadata": {
        "id": "Lbo_uA3xdCPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args=TrainingArguments(\n",
        "    output_dir='./result',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "training_args\n",
        "\n"
      ],
      "metadata": {
        "id": "dA8_B95PdJ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step5:initialize the model"
      ],
      "metadata": {
        "id": "rrK_2QZfeBPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "c59wim5UfWAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1️⃣ Load dataset (IMDB as example)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 2️⃣ Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 3️⃣ Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 4️⃣ Load model for sequence classification\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# 5️⃣ Define training arguments (compatible with older transformers)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,   # reduce epochs for testing\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "# 6️⃣ Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"]\n",
        ")\n",
        "\n",
        "# 7️⃣ Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# 8️⃣ Save the fine-tuned model and tokenizer\n",
        "trainer.save_model(\"my_fine_tuned_bert\")\n",
        "tokenizer.save_pretrained(\"my_fine_tuned_bert\")\n"
      ],
      "metadata": {
        "id": "CnAjOF_Qd1Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "e2Us5mDRiTgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "YJuW--2sQ_54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "aiclA6_HRKVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "# Define the query\n",
        "query = 'ai OR \"artificial intelligence\" OR \"machine learning\"'\n",
        "# Initialize the search with parameters\n",
        "search = arxiv.Search(\n",
        "    query=query,\n",
        "    max_results=10,\n",
        "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "# List to store paper details\n",
        "papers = []\n",
        "# Loop through results\n",
        "for result in search.results():\n",
        "    papers.append({\n",
        "        'published': result.published,\n",
        "        'title': result.title,\n",
        "        'abstract': result.summary,\n",
        "        'categories': result.categories\n",
        "    })\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "# Show full text in columns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "# Display top 10 papers\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "dXVzTx-cROwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Take first abstract from your DataFrame\n",
        "abstract = df['abstract'][0]\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Generate summary\n",
        "summarization_result = summarizer(\n",
        "    abstract,\n",
        "    max_length=130,   # summary max tokens\n",
        "    min_length=30,    # summary min tokens\n",
        "    do_sample=False   # deterministic summary\n",
        ")\n",
        "print(summarization_result[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "FNj9SA2mTbDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarization_result[0]['summary_text'])"
      ],
      "metadata": {
        "id": "GAdEiWcRUTIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syq9gBmAUVNR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}